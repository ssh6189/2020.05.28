{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 5106491044666976472, name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 12993457698636891749\n",
       " physical_device_desc: \"device: XLA_CPU device\", name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 9076743896476337425\n",
       " physical_device_desc: \"device: XLA_GPU device\", name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 14861955892\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 1575440685062195342\n",
       " physical_device_desc: \"device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "for i in tf.config.list_physical_devices():\n",
    "    print(i)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As crop_img and crop_img_to functions are missing in utils.py, we deactivated crop-related functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T03:09:16.687724Z",
     "start_time": "2020-05-26T03:09:16.646694Z"
    }
   },
   "outputs": [],
   "source": [
    "#unet3d/utils/sitk_utils.py\n",
    "#여기는 전처리 부분, 복셀간 원점, 좌표, 스페이싱, 방향, 보간법, transform, data <--> image 등을 하는 함수들을 정의한 모듈\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "\n",
    "#복셀 원점 좌표 보정\n",
    "def calculate_origin_offset(new_spacing, old_spacing):\n",
    "    return np.subtract(new_spacing, old_spacing)/2\n",
    "\n",
    "#복셀간 공간 재조정(이미지, 스페이싱 (1, 1, 1), 선형보간법 사용, 기본값 = 0)\n",
    "#즉, 복셀간 공간을 재조정하여, 이미지 재조정하는 함수에 파라미터를 넘겨주고, 호출하는 함수\n",
    "def sitk_resample_to_spacing(image, new_spacing=(1.0, 1.0, 1.0), interpolator=sitk.sitkLinear, default_value=0.):\n",
    "    #줌팩터 = 기존 이미지로부터 얻은 공간간의 간격/새이미지로 부터 얻은 공간간의 간격\n",
    "    zoom_factor = np.divide(image.GetSpacing(), new_spacing)\n",
    "    #새로운 사이즈 = np.asarray(올림(반올림(줌팩터 * 이미지로부터 얻은 사이즈)), 소수점 다섯째짜리까지)\n",
    "    new_size = np.asarray(np.ceil(np.round(np.multiply(zoom_factor, image.GetSize()), decimals=5)), dtype=np.int16)\n",
    "    #오프셋 = (새로 만들 이미지의 복셀 간격, 이미지로부터 얻은 복셀 간격)\n",
    "    offset = calculate_origin_offset(new_spacing, image.GetSpacing())\n",
    "    #참조 이미지 = 빈이미지(새 사이즈, 새 복셀간 공간, 이미지 방향, 원점+(보정값), 디폴트값)\n",
    "    reference_image = sitk_new_blank_image(size=new_size, spacing=new_spacing, direction=image.GetDirection(),\n",
    "                                           origin=image.GetOrigin() + offset, default_value=default_value)\n",
    "    #이미지 재조정(이미지, 참조이미지, 보간법, 디폴트값)\n",
    "    return sitk_resample_to_image(image, reference_image, interpolator=interpolator, default_value=default_value)\n",
    "\n",
    "#이미지 재조정(이미지, 참조된 이미지, 디폴트값, 선형보간법, transform=None, output_pixel_type=None)\n",
    "def sitk_resample_to_image(image, reference_image, default_value=0., interpolator=sitk.sitkLinear, transform=None,\n",
    "                           output_pixel_type=None):\n",
    "    #transform에 아무값도 안넣어주면, sitk에 있는 Transform 사용 후, 항등행렬로 변환\n",
    "    if transform is None:\n",
    "        transform = sitk.Transform()\n",
    "        transform.SetIdentity()\n",
    "    #output_pixel 타입에 아무값도 안 넣어주면, 이미지의 GetPixelID()값(픽셀타입을 얻어오는함수)을 output_pixel_type에 대입하여 사용\n",
    "    if output_pixel_type is None:\n",
    "        output_pixel_type = image.GetPixelID()\n",
    "    resample_filter = sitk.ResampleImageFilter() #resample_filter에 sitkResampleImageFilter 적용\n",
    "    resample_filter.SetInterpolator(interpolator) #선형보간법 설정\n",
    "    resample_filter.SetTransform(transform) #transform 설정\n",
    "    resample_filter.SetOutputPixelType(output_pixel_type) #픽셀타입설정\n",
    "    resample_filter.SetDefaultPixelValue(default_value) #픽셀값 디폴트값으로 설정\n",
    "    resample_filter.SetReferenceImage(reference_image) #참조된 이미지 값에 적용\n",
    "    return resample_filter.Execute(image) #이미지 재조정 필터 실행\n",
    "\n",
    "#빈 복셀이미지 형성(사이즈, 복셀간의 공간, 방향, 원점, 디폴트값=0)\n",
    "def sitk_new_blank_image(size, spacing, direction, origin, default_value=0.):\n",
    "    #이미지 = 1로 구성된, 이미지배열을 받아, 전치행렬을 구한후, default값을 곱해서, 이미지 Array를 받아 image에 저장\n",
    "    image = sitk.GetImageFromArray(np.ones(size, dtype=np.float).T * default_value)\n",
    "    #복셀 이미지의 스페이싱, 다이렉션, 원점 정의\n",
    "    image.SetSpacing(spacing)\n",
    "    image.SetDirection(direction)\n",
    "    image.SetOrigin(origin)\n",
    "    return image\n",
    "\n",
    "#공간 재조정(데이터, 공간, 타겟으로 하는 공간, 선형보간법, 디폴트값)\n",
    "def resample_to_spacing(data, spacing, target_spacing, interpolation=\"linear\", default_value=0.):\n",
    "    #다음 함수를 실행해, image에 저장, 데이터를 이미지로 변환시켜, image에 저장\n",
    "    image = data_to_sitk_image(data, spacing=spacing)\n",
    "    #만약, 선형보간법을 사용할 경우\n",
    "    if interpolation is \"linear\":\n",
    "        interpolator = sitk.sitkLinear\n",
    "    #만약, 최단입점보간법을 사용할 경우(즉, output을 만들때, input에서 가장 가까운 놈(비슷한 놈)을 가져다 쓴다.)\n",
    "    elif interpolation is \"nearest\":\n",
    "        interpolator = sitk.sitkNearestNeighbor\n",
    "    #그외 값들은 에러처리\n",
    "    else:\n",
    "        raise ValueError(\"'interpolation' must be either 'linear' or 'nearest'. '{}' is not recognized\".format(\n",
    "            interpolation))\n",
    "    resampled_image = sitk_resample_to_spacing(image, new_spacing=target_spacing, interpolator=interpolator,\n",
    "                                               default_value=default_value)\n",
    "    return sitk_image_to_data(resampled_image)\n",
    "\n",
    "#data -> sitk_image\n",
    "def data_to_sitk_image(data, spacing=(1., 1., 1.)):\n",
    "    #3차원이면, 다음과 같이 회전\n",
    "    if len(data.shape) == 3:\n",
    "        data = np.rot90(data, 1, axes=(0, 2))\n",
    "    #data array를 받아, image에 저장\n",
    "    image = sitk.GetImageFromArray(data)\n",
    "    image.SetSpacing(np.asarray(spacing, dtype=np.float))\n",
    "    return image\n",
    "\n",
    "#sitk_image -> data\n",
    "def sitk_image_to_data(image):\n",
    "    #data에 이미지Array값을 저장\n",
    "    data = sitk.GetArrayFromImage(image)\n",
    "    #3차원이면, 다음과 같이 회전\n",
    "    if len(data.shape) == 3:\n",
    "        data = np.rot90(data, -1, axes=(0, 2))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T03:09:17.069937Z",
     "start_time": "2020-05-26T03:09:16.688693Z"
    }
   },
   "outputs": [],
   "source": [
    "#unet3d/utils/utils.py\n",
    "#이미지 파일들을 resize해서, 이미지들이나, 이미지 파일을 읽어오는 함수 정의\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "import collections\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from nilearn.image import reorder_img, new_img_like\n",
    "\n",
    "#얘는 모르겠다.... 별로 건드리고 싶지도 않고, 건드릴 필요도 없을듯, shape형태를 교정해주는것인듯\n",
    "def fix_shape(image):\n",
    "    if image.shape[-1] == 1:\n",
    "        return image.__class__(dataobj=np.squeeze(image.get_data()), affine=image.affine)\n",
    "    return image\n",
    "\n",
    "#resize(이미지, 원하는 shape, 선형보간법)\n",
    "def resize(image, new_shape, interpolation=\"linear\"):\n",
    "    #아핀 대각선이 있는 이미지를 반환, 함수에서 반환되는 새로운 이미지의 방향은 RAS(Right, Anterior, Superior로 정의)\n",
    "    #너무 수학적인 내용이 들어갔다....\n",
    "    image = reorder_img(image, resample=interpolation)\n",
    "    #zoom 배율\n",
    "    zoom_level = np.divide(new_shape, image.shape)\n",
    "    #image.header.get_zooms()는 image의 복셀 크기를 mm단위로 얻는 함수, (2.0, 2.0, 2.19999..., 2000.0) 이런식으로 나온다는데,\n",
    "    #마지막 값은, 카메라 스캔간격을 표시한다고 한다. 스캔간격은 (miliseconds)\n",
    "    new_spacing = np.divide(image.header.get_zooms(), zoom_level)\n",
    "    #resample_to_spacing의 정의는 위의것을 참조, (image의 데이터, image의 복셀크기, newspacing, 보간법)\n",
    "    new_data = resample_to_spacing(image.get_data(), image.header.get_zooms(), new_spacing,\n",
    "                                   interpolation=interpolation)\n",
    "    #새 affine공간에 image공간 복사(affine공간이라고\n",
    "    #기하학에서, 아핀 공간은 유클리드 공간의 아핀 기하학적 성질들을 일반화해서 만들어지는 구조이다.\n",
    "    #핀 공간에서는 점에서 점을 빼서 벡터를 얻거나 점에 벡터를 더해 다른 점을 얻을 수는 있지만 원점이 없으므로 \n",
    "    #점과 점을 더할 수는 없다. 이렇게 나와있는데, 알듯말듯 한데, 체감이 잘 안옴\n",
    "    new_affine = np.copy(image.affine)\n",
    "    \n",
    "    #tolist는 배열을 목록으로 바꿔주는 함수\n",
    "    np.fill_diagonal(new_affine, new_spacing.tolist() + [1])\n",
    "    \n",
    "    #new_affine공간의 좌표를 보정\n",
    "    new_affine[:3, 3] += calculate_origin_offset(new_spacing, image.header.get_zooms())\n",
    "    \n",
    "    #참조 이미지와 동일한 클래스의 새 이미지를 만들어 반환, (참조이미지, 만들 이미지에 저장될 데이터, affin=new_affine행렬 적용)\n",
    "    return new_img_like(image, new_data, affine=new_affine)\n",
    "\n",
    "#이미지를 읽는함수, (파일, image_shape=None, 선형보간법, 추출)\n",
    "def read_image(in_file, image_shape=None, interpolation='linear', crop=None):\n",
    "    print(\"Reading: {0}\".format(in_file))\n",
    "    #파일 로드\n",
    "    image = nib.load(in_file) # J.Lee: nib.load(os.path.abspath(in_file)) -> nib.load(in_file)\n",
    "    image = fix_shape(image)\n",
    "#     if crop:\n",
    "#         image = crop_img_to(image, crop, copy=True)\n",
    "    #image_shape가 true, 즉, shape값이 있으면, resize함수 적용후 반환, 아니면, 그냥 반환\n",
    "    if image_shape:\n",
    "        return resize(image, new_shape=image_shape, interpolation=interpolation)\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "#이미지 파일들을 읽는 함수\n",
    "\n",
    "#함수에 들어있는 파라미터 의미는 다음과 같다.\n",
    "#image_files : 이미지파일들 경로\n",
    "#image_shape : 이미지 형태, 가로*세로*높이 = (240, 240, 155)\n",
    "#crop : 추출여부\n",
    "#label_indices : 예상되는 라벨이미지의 인덱스(1, 2, 4)\n",
    "#선형보간법을 사용하면, 레이블이 엉망이 되니, 건들지말라고 써있다!\n",
    "\n",
    "def read_image_files(image_files, image_shape=None, crop=None, label_indices=None):\n",
    "    \"\"\"\n",
    "    :param image_files: # J.Lee: paths of image_files.\n",
    "    :param image_shape: # J.Lee: (d,l,m)(155,240,240)\n",
    "    # J.Lee:\n",
    "    :param label_indices:\n",
    "        len(image_files)-1. i.e. expected index of label(truth) image \n",
    "        (when called through: write_image_data_to_file-> reslice_image-> read_image_files)\n",
    "    :param crop: \n",
    "    :param use_nearest_for_last_file: If True, will use nearest neighbor interpolation for the last file. This is used\n",
    "    because the last file may be the labels file. Using linear interpolation here would mess up the labels.\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if label_indices is None:\n",
    "        label_indices = []\n",
    "    elif not isinstance(label_indices, collections.Iterable) or isinstance(label_indices, str):\n",
    "        label_indices = [label_indices]\n",
    "    image_list = list()\n",
    "    for index, image_file in enumerate(image_files):\n",
    "        if (label_indices is None and (index + 1) == len(image_files)) \\\n",
    "                or (label_indices is not None and index in label_indices):\n",
    "            interpolation = \"nearest\"\n",
    "        else:\n",
    "            interpolation = \"linear\"\n",
    "        image_list.append(read_image(image_file, image_shape=image_shape, crop=crop, interpolation=interpolation))\n",
    "\n",
    "    return image_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T03:09:17.077909Z",
     "start_time": "2020-05-26T03:09:17.070936Z"
    }
   },
   "outputs": [],
   "source": [
    "#unet3d/normalize.py\n",
    "#슬라이스와 정규화 함수 정의\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from nilearn.image import new_img_like\n",
    "\n",
    "# from unet3d.utils.utils import resize, read_image_files\n",
    "# from .utils import crop_img, crop_img_to, read_image #!!!Missing in .utils!!!\n",
    "\n",
    "# def get_cropping_parameters(in_files):\n",
    "#     if len(in_files) > 1:\n",
    "#         foreground = get_complete_foreground(in_files)\n",
    "#     else:\n",
    "#         foreground = get_foreground_from_set_of_files(in_files[0], return_image=True)\n",
    "#     return crop_img(foreground, return_slices=True, copy=True)\n",
    "\n",
    "#슬라이스\n",
    "def reslice_image_set(in_files, image_shape, out_files=None, label_indices=None, crop=False):\n",
    "    if crop:\n",
    "        pass\n",
    "#         crop_slices = get_cropping_parameters([in_files])\n",
    "    else:\n",
    "        crop_slices = None\n",
    "    #이미지 파일들을 불러와서   \n",
    "    images = read_image_files(in_files, image_shape=image_shape, crop=crop_slices, label_indices=label_indices)\n",
    "    #Out_files가 None이 아닐경우, 즉, 존재할 경우, 다음을 실행\n",
    "    if out_files:\n",
    "        for image, out_file in zip(images, out_files):\n",
    "            #모든 이미지마다, filename을 붙여줌, 즉, out_file과 매칭\n",
    "            image.to_filename(out_file)\n",
    "        return [out_file for out_file in out_files]\n",
    "        # J.Lee: original: [os.path.abspath(out_file) for out_file in out_files]\n",
    "    else:\n",
    "        return images\n",
    "\n",
    "#data 정규화\n",
    "def normalize_data(data, mean, std):\n",
    "    #newaxis는 간단히 말해, 차원을 늘려주는것이라고 생각하면 된다.\n",
    "    #data = data - mean[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "    data -= mean[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "    #data = data / std[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "    data /= std[:, np.newaxis, np.newaxis, np.newaxis]\n",
    "    return data\n",
    "\n",
    "#정규화된 데이터를 저장\n",
    "def normalize_data_storage(data_storage):\n",
    "    #빈 리스트 생성\n",
    "    means = list()\n",
    "    stds = list()\n",
    "    \n",
    "    for index in range(data_storage.shape[0]):\n",
    "        data = data_storage[index]\n",
    "        means.append(data.mean(axis=(1, 2, 3)))\n",
    "        stds.append(data.std(axis=(1, 2, 3)))\n",
    "    mean = np.asarray(means).mean(axis=0)\n",
    "    std = np.asarray(stds).mean(axis=0)\n",
    "    for index in range(data_storage.shape[0]):\n",
    "        data_storage[index] = normalize_data(data_storage[index], mean, std)\n",
    "    return data_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T03:09:17.123936Z",
     "start_time": "2020-05-26T03:09:17.078908Z"
    }
   },
   "outputs": [],
   "source": [
    "#unet3d/data.py\n",
    "#hdf5파일을 생성 함수와, 생성된 hdf5파일에 데이터셋 기록 및 추가, 읽어오는 함수가 정의된 부분인듯\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import tables\n",
    "\n",
    "#from .normalize import normalize_data_storage, reslice_image_set\n",
    "\n",
    "#데이터 파일 생성 함수 정의\n",
    "def create_data_file(out_file, n_channels, n_samples, image_shape):\n",
    "    #hdf5파일 쓰기모드\n",
    "    hdf5_file = tables.open_file(out_file, mode='w')\n",
    "    #complevel : 필터속성의 컨테이너(데이터의 압축 수준을 지정, 허용되는 범위는 0-9, 값 0 (기본값)은 압축을 비활성화)\n",
    "    #complib : 사용할 압축 라이브러리\n",
    "    filters = tables.Filters(complevel=5, complib='blosc')\n",
    "    data_shape = tuple([0, n_channels] + list(image_shape)) # J.Lee: (155,240,240) -> [0,4,155,240,240]\n",
    "    truth_shape = tuple([0, 1] + list(image_shape)) # J.Lee: (155,240,240) -> [0,1,155,240,240]\n",
    "    \n",
    "    #딥러닝 훈련용, 하둡파일 생성 (본 코드에서는 hdf5로 생성)\n",
    "    data_storage = hdf5_file.create_earray(hdf5_file.root, 'data', tables.Float32Atom(), shape=data_shape,\n",
    "                                           filters=filters, expectedrows=n_samples)\n",
    "    truth_storage = hdf5_file.create_earray(hdf5_file.root, 'truth', tables.UInt8Atom(), shape=truth_shape,\n",
    "                                            filters=filters, expectedrows=n_samples)\n",
    "    affine_storage = hdf5_file.create_earray(hdf5_file.root, 'affine', tables.Float32Atom(), shape=(0, 4, 4),\n",
    "                                             filters=filters, expectedrows=n_samples)\n",
    "    return hdf5_file, data_storage, truth_storage, affine_storage\n",
    "\n",
    "#파일에 이미지 데이터 기록\n",
    "def write_image_data_to_file(image_files, data_storage, truth_storage, image_shape, n_channels, affine_storage,\n",
    "                             truth_dtype=np.uint8, crop=True):\n",
    "    for set_of_files in image_files:\n",
    "        # J.Lee: {set of files} includes paths of 1~4 channel images + 1truth image\n",
    "        #슬라이스된 이미셋을 images에 저장\n",
    "        images = reslice_image_set(set_of_files, image_shape, label_indices=len(set_of_files) - 1, crop=crop)\n",
    "        #images에 저장된 이미지에서 data를 뽑아내, subject_data에 저장\n",
    "        subject_data = [image.get_data() for image in images]\n",
    "        #생덩된 하둡파일에 데이터 추가\n",
    "        add_data_to_storage(data_storage, truth_storage, affine_storage, subject_data, images[0].affine, n_channels,\n",
    "                            truth_dtype)\n",
    "    return data_storage, truth_storage\n",
    "\n",
    "#생성된 하둡파일에 데이터 추가\n",
    "#그냥, 각각 세 list에 각각 추가하는것\n",
    "def add_data_to_storage(data_storage, truth_storage, affine_storage, subject_data, affine, n_channels, truth_dtype):\n",
    "    data_storage.append(np.asarray(subject_data[:n_channels])[np.newaxis])\n",
    "    truth_storage.append(np.asarray(subject_data[n_channels], dtype=truth_dtype)[np.newaxis][np.newaxis])\n",
    "    affine_storage.append(np.asarray(affine)[np.newaxis])\n",
    "\n",
    "\n",
    "#hdf5 파일에 data 기록\n",
    "#학습 이미지 세트를 가져와서 해당 이미지를 hdf5 파일에 기록한다.\n",
    "#함수에 들어있는 파라미터의 의미는 다음과 같다.\n",
    "# training_data_files : 훈련 데이터 파일을 포함하는 튜플 목록. modality(t1, t2, t1ce, flair)은 튜플내에서 다음과 같은 순서로 나열되어야한다.\n",
    "# 마지막 last item은 각 튜플에서 라벨링된 이미지다. 그래서, 1을 빼준다.\n",
    "\n",
    "#outfile : hdf5 파일이 쓰일 경로\n",
    "#image_shape : hdf5파일에 저장될 이미지의 shape\n",
    "#truth_dtype : default = 8-bit 양의 정수로 설정\n",
    "\n",
    "def write_data_to_file(training_data_files, out_file, image_shape, truth_dtype=np.uint8, subject_ids=None,\n",
    "                       normalize=True, crop=False): # J.Lee: crop=True -> crop=False\n",
    "    \"\"\"\n",
    "    Takes in a set of training images and writes those images to an hdf5 file.\n",
    "    :param training_data_files: List of tuples containing the training data files. The modalities should be listed in\n",
    "    the same order in each tuple. The last item in each tuple must be the labeled image. \n",
    "    Example: [('sub1-T1.nii.gz', 'sub1-T2.nii.gz', 'sub1-truth.nii.gz'), \n",
    "              ('sub2-T1.nii.gz', 'sub2-T2.nii.gz', 'sub2-truth.nii.gz')]\n",
    "    :param out_file: Where the hdf5 file will be written to.\n",
    "    :param image_shape: Shape of the images that will be saved to the hdf5 file.\n",
    "    :param truth_dtype: Default is 8-bit unsigned integer.\n",
    "    :return: Location of the hdf5 file with the image data written to it. \n",
    "    \"\"\"\n",
    "    n_samples = len(training_data_files)\n",
    "    n_channels = 4 #SSH: len(training_data_files[0]) - 1, t1, t1ce, t2, flair, label된 이미지[seg], 그래서, 1을 빼준다.\n",
    "    \n",
    "    #이상 없으면, 데이터 파일 생성 함수에 파라미터 값을 넘겨주고, 함수를 실행후, 리턴된 값을, 각각 파일에 저장\n",
    "    try:\n",
    "        hdf5_file, data_storage, truth_storage, affine_storage = create_data_file(out_file,\n",
    "                                                                                  n_channels=n_channels,\n",
    "                                                                                  n_samples=n_samples,\n",
    "                                                                                  image_shape=image_shape)\n",
    "    #예외 발생시, 미완성된 datafile을 삭제, 그래서 예전에 만들어놓은 hdf5파일이 없어진듯\n",
    "    except Exception as e:\n",
    "        # If something goes wrong, delete the incomplete data file\n",
    "        os.remove(out_file)\n",
    "        raise e\n",
    "\n",
    "    #파일에 이미지 데이터 기록\n",
    "    write_image_data_to_file(training_data_files, data_storage, truth_storage, image_shape,\n",
    "                             truth_dtype=truth_dtype, n_channels=n_channels, affine_storage=affine_storage, crop=crop)\n",
    "    #subject_ids값이 들어오면, hdf5_file에 subject_ids라는 \n",
    "    if subject_ids:\n",
    "        hdf5_file.create_array(hdf5_file.root, 'subject_ids', obj=subject_ids)\n",
    "    #normalize값이 들어오면, datastorage 부분을 정규화 시키고, hdf5파일 닫기\n",
    "    if normalize:\n",
    "        normalize_data_storage(data_storage)\n",
    "    hdf5_file.close()\n",
    "    \n",
    "    return out_file\n",
    "\n",
    "#읽기모드로, 테이블파일 열기\n",
    "def open_data_file(filename, readwrite=\"r\"):\n",
    "    return tables.open_file(filename, readwrite)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T03:09:17.157936Z",
     "start_time": "2020-05-26T03:09:17.124936Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/lab10/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TCIA03_257_1/BraTS19_TCIA03_257_1_flair.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/lab10/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TCIA03_257_1/BraTS19_TCIA03_257_1_t1ce.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/lab10/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TCIA03_257_1/BraTS19_TCIA03_257_1_t1.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/lab10/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TCIA03_257_1/BraTS19_TCIA03_257_1_t2.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/lab10/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TCIA03_257_1/BraTS19_TCIA03_257_1_seg.nii.gz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        0\n",
       "0  /home/lab10/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TCIA03_257_1/BraTS19_TCIA03_257_1_flair.nii.gz\n",
       "1   /home/lab10/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TCIA03_257_1/BraTS19_TCIA03_257_1_t1ce.nii.gz\n",
       "2     /home/lab10/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TCIA03_257_1/BraTS19_TCIA03_257_1_t1.nii.gz\n",
       "3     /home/lab10/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TCIA03_257_1/BraTS19_TCIA03_257_1_t2.nii.gz\n",
       "4    /home/lab10/MICCAI_BraTS_2019_Data_Training/HGG/BraTS19_TCIA03_257_1/BraTS19_TCIA03_257_1_seg.nii.gz"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/lab10/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TCIA13_621_1/BraTS19_TCIA13_621_1_flair.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/lab10/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TCIA13_621_1/BraTS19_TCIA13_621_1_t1.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/lab10/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TCIA13_621_1/BraTS19_TCIA13_621_1_t1ce.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/lab10/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TCIA13_621_1/BraTS19_TCIA13_621_1_t2.nii.gz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/home/lab10/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TCIA13_621_1/BraTS19_TCIA13_621_1_seg.nii.gz</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        0\n",
       "0  /home/lab10/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TCIA13_621_1/BraTS19_TCIA13_621_1_flair.nii.gz\n",
       "1     /home/lab10/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TCIA13_621_1/BraTS19_TCIA13_621_1_t1.nii.gz\n",
       "2   /home/lab10/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TCIA13_621_1/BraTS19_TCIA13_621_1_t1ce.nii.gz\n",
       "3     /home/lab10/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TCIA13_621_1/BraTS19_TCIA13_621_1_t2.nii.gz\n",
       "4    /home/lab10/MICCAI_BraTS_2019_Data_Training/LGG/BraTS19_TCIA13_621_1/BraTS19_TCIA13_621_1_seg.nii.gz"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#modified from N.Freidman's preprocessing code\n",
    "#나오미코드를 참조해서 만든 전처리\n",
    "#우리가 가진 데이터 셋을 분류 및 원하는 순서대로 정렬하고, 트레이닝 데이터 파일에 저장\n",
    "\n",
    "data_dir = '/home/lab10/MICCAI_BraTS_2019_Data_Training/'\n",
    "DATA_HGG = data_dir+'HGG/'\n",
    "DATA_LGG = data_dir+'LGG/'\n",
    "\n",
    "#modality(mri 사진 종류를다음과 같은 순서로 정렬)\n",
    "def modality_sorter(file_path, modality_order = ['flair', 't1','t1ce', 't2', 'seg']):\n",
    "    '''used as key to sort {file path} by {modality_order}'''\n",
    "    for i, modality in enumerate(modality_order):\n",
    "        if modality in file_path: return i\n",
    "\n",
    "#다음과 같이 빈배열을 만들고, BraTS19라는 이름이 경로내에 있으며, nii.gz라는 명을 가지고, nb4라는 명이 없으면, filepaths에 저장 \n",
    "#아까 정의한 modality order대로, 정렬후,hgg_path와 lgg_path에 추가\n",
    "hgg_paths = []\n",
    "for dirpath, dirnames, files in os.walk(DATA_HGG):\n",
    "    if ('BraTS19' in dirpath):\n",
    "        file_paths = [dirpath+'/'+file for file in files if ('nii.gz' in file) and ('nb4' not in file)] # J.Lee: include N4Bias images if you want\n",
    "        file_paths.sort(key=modality_sorter)\n",
    "        hgg_paths.append(tuple(file_paths))\n",
    "\n",
    "lgg_paths = []\n",
    "for dirpath, dirnames, files in os.walk(DATA_LGG):\n",
    "    if ('BraTS19' in dirpath):\n",
    "        file_paths = [dirpath+'/'+file for file in files if ('nii.gz' in file) and ('nb4' not in file)] # J.Lee: include N4Bias images if you want\n",
    "        file_paths.sort(key=modality_sorter)\n",
    "        lgg_paths.append(tuple(file_paths))\n",
    "\n",
    "training_data_files = hgg_paths+lgg_paths\n",
    "\n",
    "#데이타 프레임을 보기 위한 코드\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "pd.set_option('max_colwidth', 120)\n",
    "\n",
    "display(pd.DataFrame(training_data_files[1]))\n",
    "display(pd.DataFrame(training_data_files[-1]))\n",
    "\n",
    "out_file = data_dir+'htf5/'\n",
    "image_shape = (155,240,240)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "#t1, t1ce1 t2, flair, seg가 전부 들어있는지 보기위한것, 큰 의미는 없다.\n",
    "for i in training_data_files:\n",
    "    print(len(i))\n",
    "    if len(i) != 5:\n",
    "        print(\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T03:21:58.672307Z",
     "start_time": "2020-05-26T03:09:17.158908Z"
    }
   },
   "outputs": [],
   "source": [
    "#def write_data_to_file(training_data_files, out_file, image_shape, truth_dtype=np.uint8, subject_ids=None,\n",
    "#                        normalize=True, crop=False):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#위에서 정의한, write_data_to_file을 실행해, 해당 경로에, h5_outfiles.h5 파일 생성, 파일이 존재하면 생략해도 무방\n",
    "training_data_files = training_data_files\n",
    "outfiles = '/home/lab10/MICCAI_BraTS_2019_Data_Training/h5_outfiles.h5'\n",
    "image_shape = (155,240,240)\n",
    "\n",
    "#write_data_to_file(training_data_files, outfiles, image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T05:29:45.678511Z",
     "start_time": "2020-05-26T05:29:45.611510Z"
    }
   },
   "outputs": [],
   "source": [
    "#generator related functions\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "#utils/utils.py\n",
    "#pickle 함수 정의\n",
    "\n",
    "#pickle write\n",
    "def pickle_dump(item, out_file):\n",
    "    with open(out_file, \"wb\") as opened_file:\n",
    "        pickle.dump(item, opened_file)\n",
    "\n",
    "#pickle load\n",
    "def pickle_load(in_file):\n",
    "    with open(in_file, \"rb\") as opened_file:\n",
    "        return pickle.load(opened_file)\n",
    "\n",
    "#utils/patches.py\n",
    "# import numpy as np\n",
    "#patch 인덱스 연산 및, 코너나 경계값에서, patch를 어떻게 처리할지에 대해 정의 된 함수가 들어있다.\n",
    "\n",
    "#patch 인덱스 연산함수(image_shape, patch_size, overlap, start=None)\n",
    "#image_shape : (155, 240, 240)\n",
    "#patch_shape : 이미지 shape를 탐색하면서 훑어볼 작은 상자\n",
    "#overlap : patch간의 겹치는부분\n",
    "def compute_patch_indices(image_shape, patch_size, overlap, start=None):\n",
    "    #overlap 변수가 int형이면, 실행하여,overlap값을 결정 \n",
    "    if isinstance(overlap, int):\n",
    "        #overlap = np.asarray([overlap]*len[155, 240, 240]), 다음과 같이 연산하여, 겹치는 부분을 결정\n",
    "        overlap = np.asarray([overlap] * len(image_shape))\n",
    "        \n",
    "    #start가 None이면, 실행\n",
    "    if start is None:\n",
    "        #(image_shape/patch_size-overlap) 연산 후 올림하여, n_patches에 저장\n",
    "        #즉, 패치들의 개수 = 전체 3D이미지/patch들간의 합집합 연산 후 반올림\n",
    "        #그냥, patch들의 수 구하는 공식\n",
    "        n_patches = np.ceil(image_shape / (patch_size - overlap))\n",
    "        #(patch_size - overlap) * n_patches - image_shape + overlap 연산 후 overflow에 저장\n",
    "        #즉, overflow는 patch들간의 겹치지 않은 부분들간의 집합 + 겹치는 부분들의 집합 - image 전체 모양\n",
    "        overflow = (patch_size - overlap) * n_patches - image_shape + overlap\n",
    "        #overflow/2한 값을 반올림 한후 -부호를 붙여, start 값으로 지정\n",
    "        start = -np.ceil(overflow/2)\n",
    "    #start 값이 정상적으로 정수값으로 존재하면\n",
    "    elif isinstance(start, int):\n",
    "        #다음과 같이 연산하여, 시작지점을 결정\n",
    "        start = np.asarray([start] * len(image_shape))\n",
    "    #stop은 탐색 종료부분\n",
    "    stop = image_shape + start\n",
    "    #step은 patch로 전체 이미지를 훑는데, 몇단계가 걸리느냐?\n",
    "    step = patch_size - overlap\n",
    "    #구한, start, stop, step함수로 get_set_of_patch_indices함수를 호출\n",
    "    return get_set_of_patch_indices(start, stop, step)\n",
    "\n",
    "#start, stop, step 값을 받아, 각 axis별로, 값을 세팅한 후에, 정수로 되어있는 전치행렬로 만든후, np.mgird로 차원뻥튀기를 해준다.\n",
    "#만약, start가 0, stop이 10이고, step이 5면, 0, 2, 4, 6, 8값을 가지고, step이 2이면, 0, 5 이런 값을 가진다.\n",
    "#step값 미지정시, 0, 1, 2, 3 .... 9까지 증가한다.\n",
    "def get_set_of_patch_indices(start, stop, step):\n",
    "    return np.asarray(np.mgrid[start[0]:stop[0]:step[0], start[1]:stop[1]:step[1],\n",
    "                               start[2]:stop[2]:step[2]].reshape(3, -1).T, dtype=np.int)\n",
    "\n",
    "#patch index를 임의로 얻어온다.\n",
    "def get_random_patch_index(image_shape, patch_shape):\n",
    "    #patch의 코너 인덱스를 랜덤하게 반환한다. 만약, training중 이 함수를 사용하면, 중간 픽셀이 가장자리 픽셀보다 더 자주 보이게 될것이다.\n",
    "    #이건 나쁜 일이니, 트레이닝 중에 사용하지 말라는것 같다.\n",
    "    \"\"\"\n",
    "    Returns a random corner index for a patch. If this is used during training, the middle pixels will be seen by\n",
    "    the model way more often than the edge pixels (which is probably a bad thing).\n",
    "    :param image_shape: Shape of the image\n",
    "    :param patch_shape: Shape of the patch\n",
    "    :return: a tuple containing the corner index which can be used to get a patch from an image\n",
    "    \"\"\"\n",
    "    #image_shape와 patch_shape의 차를 구한후 랜덤하게 반환한다.\n",
    "    return get_random_nd_index(np.subtract(image_shape, patch_shape))\n",
    "\n",
    "#nd index를 임의로 얻어온다.\n",
    "def get_random_nd_index(index_max):\n",
    "    return tuple([np.random.choice(index_max[index] + 1) for index in range(len(index_max))])\n",
    "\n",
    "#3d 데이터에서 patch를 얻어온다.\n",
    "\n",
    "#data : patch에서 얻어온 numpy array\n",
    "#patch_shape : patch의 모양/사이즈\n",
    "#patch_index : patch의 코너인덱스\n",
    "def get_patch_from_3d_data(data, patch_shape, patch_index):\n",
    "    \"\"\"\n",
    "    Returns a patch from a numpy array.\n",
    "    :param data: numpy array from which to get the patch.\n",
    "    :param patch_shape: shape/size of the patch.\n",
    "    :param patch_index: corner index of the patch.\n",
    "    :return: numpy array take from the data with the patch shape specified.\n",
    "    \"\"\"\n",
    "    patch_index = np.asarray(patch_index, dtype=np.int16)\n",
    "    patch_shape = np.asarray(patch_shape)\n",
    "    image_shape = data.shape[-3:]\n",
    "    \n",
    "    #np.any : np배열 내부에서 조건에 맞는 데이터가 있으면 True, 아니면, False 반환\n",
    "    #즉, patch가 image의 범위에서 벗어난 부분에 대해서, 보정을 해주는 부분인듯, 즉, 작은 patch라는 상자가, 큰 상자의 바깥부분에\n",
    "    #존재할때, 그 값들을 보정하기 위하여, fix_out_of_bound_patch_attempt함수를 불러와서, 보정해주는것인듯, 그 외에는 그냥 리턴\n",
    "    if np.any(patch_index < 0) or np.any((patch_index + patch_shape) > image_shape):\n",
    "        data, patch_index = fix_out_of_bound_patch_attempt(data, patch_shape, patch_index)\n",
    "    return data[..., patch_index[0]:patch_index[0]+patch_shape[0], patch_index[1]:patch_index[1]+patch_shape[1],\n",
    "                patch_index[2]:patch_index[2]+patch_shape[2]]\n",
    "\n",
    "#경계부분 patch 수정, 이 함수는 위의, 3d_data로 부터, patch를 얻어올때만 쓰인다.\n",
    "def fix_out_of_bound_patch_attempt(data, patch_shape, patch_index, ndim=3):\n",
    "    \"\"\"\n",
    "    Pads the data and alters the patch index so that a patch will be correct.\n",
    "    :param data:\n",
    "    :param patch_shape:\n",
    "    :param patch_index:\n",
    "    :return: padded data, fixed patch index\n",
    "    \"\"\"\n",
    "    #image_shape = data.shape\n",
    "    image_shape = data.shape[-ndim:]\n",
    "    #pad_before = patch가 음수면, 제곱해서, 절대값으로 변환\n",
    "    pad_before = np.abs((patch_index < 0) * patch_index)\n",
    "    #pad_after = patch가 image_shape를 이탈하면, 다음 방법으로 값을 보정\n",
    "    pad_after = np.abs(((patch_index + patch_shape) > image_shape) * ((patch_index + patch_shape) - image_shape))\n",
    "    #pad_before, pad_after를 axis = 1로 병합\n",
    "    pad_args = np.stack([pad_before, pad_after], axis=1)\n",
    "    #만약, pad_args의 차원이 \n",
    "    #tolist는 nd array를 list로 바꾸어주는 함수\n",
    "    if pad_args.shape[0] < len(data.shape):\n",
    "        pad_args = [[0, 0]] * (len(data.shape) - pad_args.shape[0]) + pad_args.tolist()\n",
    "    #np.pad(data, padding array, edge부분으로 처리)\n",
    "    #즉, 데이터가 존재하고, padding에 edge라고 처리하는것이다.\n",
    "    #예를 들어, data = [[1,2],[3,4]], pad_args = [[2, 2], [2, 2]], mode = \"edge\"면\n",
    "    '''\n",
    "    e e e e e e\n",
    "    e e e e e e\n",
    "    e e 1 2 e e\n",
    "    e e 3 4 e e\n",
    "    e e e e e e\n",
    "    e e e e e e\n",
    "    \n",
    "    e = edge모드로 처리될 부분, 이런식으로 처리된다는 의미, 즉, 전체 이미지의 가장자리 부분을 처리하는 부분이다.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    data = np.pad(data, pad_args, mode=\"edge\")\n",
    "    patch_index += pad_before\n",
    "    #data와 patch index 반환\n",
    "    return data, patch_index\n",
    "\n",
    "#patch 목록 및 해당 patch index에서 원래 모양의 배열을 재구성합니다. 겹치는 패치는 평균화\n",
    "\n",
    "#patch들로부터 재설계\n",
    "#patches : numpy array로 된 patch들의 목록\n",
    "#patch_indicces : patch들의 목록에 해당하는 index 목록\n",
    "#patch들로부터 추출한 array의 shape\n",
    "#default값 : 결과데이터의 기본값, 만약, patch 적용이 완료되면, 이 값은 덮어씌어진다.\n",
    "\n",
    "\n",
    "#함수는 정의는 되어있는데, 사용된곳이 없어서 보류\n",
    "def reconstruct_from_patches(patches, patch_indices, data_shape, default_value=0):\n",
    "    \"\"\"\n",
    "    Reconstructs an array of the original shape from the lists of patches and corresponding patch indices. Overlapping\n",
    "    patches are averaged.\n",
    "    :param patches: List of numpy array patches.\n",
    "    :param patch_indices: List of indices that corresponds to the list of patches.\n",
    "    :param data_shape: Shape of the array from which the patches were extracted.\n",
    "    :param default_value: The default value of the resulting data. if the patch coverage is complete, this value will\n",
    "    be overwritten.\n",
    "    :return: numpy array containing the data reconstructed by the patches.\n",
    "    \"\"\"\n",
    "    data = np.ones(data_shape) * default_value\n",
    "    image_shape = data_shape[-3:]\n",
    "    count = np.zeros(data_shape, dtype=np.int)\n",
    "    for patch, index in zip(patches, patch_indices):\n",
    "        image_patch_shape = patch.shape[-3:]\n",
    "        if np.any(index < 0):\n",
    "            fix_patch = np.asarray((index < 0) * np.abs(index), dtype=np.int)\n",
    "            patch = patch[..., fix_patch[0]:, fix_patch[1]:, fix_patch[2]:]\n",
    "            index[index < 0] = 0\n",
    "        if np.any((index + image_patch_shape) >= image_shape):\n",
    "            fix_patch = np.asarray(image_patch_shape - (((index + image_patch_shape) >= image_shape)\n",
    "                                                        * ((index + image_patch_shape) - image_shape)), dtype=np.int)\n",
    "            patch = patch[..., :fix_patch[0], :fix_patch[1], :fix_patch[2]]\n",
    "        patch_index = np.zeros(data_shape, dtype=np.bool)\n",
    "        patch_index[...,\n",
    "                    index[0]:index[0]+patch.shape[-3],\n",
    "                    index[1]:index[1]+patch.shape[-2],\n",
    "                    index[2]:index[2]+patch.shape[-1]] = True\n",
    "        patch_data = np.zeros(data_shape)\n",
    "        patch_data[patch_index] = patch.flatten()\n",
    "\n",
    "        new_data_index = np.logical_and(patch_index, np.logical_not(count > 0))\n",
    "        data[new_data_index] = patch_data[new_data_index]\n",
    "\n",
    "        averaged_data_index = np.logical_and(patch_index, count > 0)\n",
    "        if np.any(averaged_data_index):\n",
    "            data[averaged_data_index] = (data[averaged_data_index] * count[averaged_data_index] + patch_data[averaged_data_index]) / (count[averaged_data_index] + 1)\n",
    "        count[patch_index] += 1\n",
    "    return data\n",
    "\n",
    "\n",
    "#######################\n",
    "#augmentaion에 사용된 함수들, scale 변경, 뒤집기, transpose등과 관련된 함수들의 정의되어 있다.\n",
    "#augment.py\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "from nilearn.image import new_img_like, resample_to_img\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "#이미지에 크기변환 후, affine 적용하는 함수\n",
    "#new_img_like : 참조 이미지와 동일한 새로운 이미지를 만드는데 사용하는 함수, (이미지, data, affine 배열행렬)\n",
    "def scale_image(image, scale_factor):\n",
    "    scale_factor = np.asarray(scale_factor)\n",
    "    new_affine = np.copy(image.affine)\n",
    "    new_affine[:3, :3] = image.affine[:3, :3] * scale_factor\n",
    "    new_affine[:, 3][:3] = image.affine[:, 3][:3] + (image.shape * np.diag(image.affine)[:3] * (1 - scale_factor)) / 2\n",
    "    return new_img_like(image, data=image.get_data(), affine=new_affine)\n",
    "\n",
    "\n",
    "#이미지와 축을 이용해, axis를 기준으로, 반전해주는 함수\n",
    "def flip_image(image, axis):\n",
    "    try:\n",
    "        new_data = np.copy(image.get_data())\n",
    "        for axis_index in axis:\n",
    "            new_data = np.flip(new_data, axis=axis_index)\n",
    "    except TypeError:\n",
    "        new_data = np.flip(image.get_data(), axis=axis)\n",
    "    return new_img_like(image, data=new_data)\n",
    "\n",
    "#아래의 augmentation data 함수에 사용하기 위해, 정의된 함수\n",
    "#len(truth.shape)값을 인자로 받게됨\n",
    "def random_flip_dimensions(n_dimensions):\n",
    "    axis = list()\n",
    "    for dim in range(n_dimensions):\n",
    "        #random_boolean을 형성한다. True or False의 값을 뱉어내게 되는데, 1이면, axis에 axis에 값을 추가한다.\n",
    "        if random_boolean():\n",
    "            axis.append(dim)\n",
    "    #결과적으로, (truth.shape의 factor들을 무작위로 axis에 집어 넣게 된다.)\n",
    "    return axis\n",
    "\n",
    "#얘도, augmentation data 함수에 사용하기 위해, 정의된 함수\n",
    "#무작위 표본 정규분표를 만든다. np.random.normal(location, size, scale)\n",
    "def random_scale_factor(n_dim=3, mean=1, std=0.25):\n",
    "    return np.random.normal(mean, std, n_dim)\n",
    "\n",
    "#위의 random_flip_dimensions에 사용하기 위해 정의된 함수\n",
    "def random_boolean():\n",
    "    return np.random.choice([True, False])\n",
    "\n",
    "#얘도, augmentation data 함수에 사용하기 위해, 정의된 함수\n",
    "#distort_image(이미지, 반전 기준 축, scale_factor), random으로 flip_axis에 append 했기 때문에, 값이 None일 수도 있고, 있을 수도 있다.\n",
    "#flip_axis 값이 있다면, 해당 axis를 기준으로 반전\n",
    "#scale_factor도 마찬가지\n",
    "#이 둘을 적용시켜, 왜곡된 이미지를 뱉어낸다.\n",
    "def distort_image(image, flip_axis=None, scale_factor=None):\n",
    "    if flip_axis:\n",
    "        image = flip_image(image, flip_axis)\n",
    "    if scale_factor is not None:\n",
    "        image = scale_image(image, scale_factor)\n",
    "    return image\n",
    "\n",
    "#위의 정의된 함수들을 다 이 함수에 적용시켜서, 데이터를 augmentation 하는 함수\n",
    "def augment_data(data, truth, affine, scale_deviation=None, flip=True):\n",
    "    n_dim = len(truth.shape)\n",
    "    if scale_deviation:\n",
    "        scale_factor = random_scale_factor(n_dim, std=scale_deviation)\n",
    "    else:\n",
    "        scale_factor = None\n",
    "    if flip:\n",
    "        flip_axis = random_flip_dimensions(n_dim)\n",
    "    else:\n",
    "        flip_axis = None\n",
    "    data_list = list()\n",
    "    \n",
    "    for data_index in range(data.shape[0]):\n",
    "        image = get_image(data[data_index], affine)\n",
    "        #datalist에 왜곡을 시킨 이미지를 재조정후에 저장, resample_to_image함수에 대한 설명은 아래에\n",
    "        #resample_to_img(source_img, target_img, interpolation = 'continuous',copy = True ,order = 'F' ,clip = False ,fill_value = 0 , force_resample = False )\n",
    "        data_list.append(resample_to_img(distort_image(image, flip_axis=flip_axis,\n",
    "                                                       scale_factor=scale_factor), image,\n",
    "                                         interpolation=\"continuous\").get_data())\n",
    "    #아래도 같음\n",
    "    data = np.asarray(data_list)\n",
    "    truth_image = get_image(truth, affine)\n",
    "    truth_data = resample_to_img(distort_image(truth_image, flip_axis=flip_axis, scale_factor=scale_factor),\n",
    "                                 truth_image, interpolation=\"nearest\").get_data()\n",
    "    return data, truth_data\n",
    "\n",
    "\n",
    "#nib.Nifti1Image(new_data, img.affine, img.header[얘는 선택사항, 없어도 됨])\n",
    "#이미지에 affine 적용후, 새로운 이미지를 생성하는 함수, 위의 augment_data함수에서 사용됨\n",
    "def get_image(data, affine, nib_class=nib.Nifti1Image):\n",
    "    return nib_class(dataobj=data, affine=affine)\n",
    "\n",
    "\n",
    "#여기는 만들어놓고 안쓴 함수들, 3D를 회전시키고, 뒤집고 하는데 사용한 함수들 정의\n",
    "\n",
    "\n",
    "#이 함수는 48 개의 고유한 회전을 나타내는 \"키\"세트를 반환합니다. 즉, 3D 회전 매트릭스를 반환\n",
    "#각 세트의 항목은 튜플로 되어있다. ((rotate_y, rotate_z), flip_x, flip_y, flip_z, transpose)\n",
    "#예를들어 ((0, 1), 0, 1, 0, 1) 이것은, z축을 중심으로 90도 회전후, y축 기준으로 뒤집고, transpose 시킨다.\n",
    "def generate_permutation_keys():\n",
    "    \"\"\"\n",
    "    This function returns a set of \"keys\" that represent the 48 unique rotations &\n",
    "    reflections of a 3D matrix.\n",
    "\n",
    "    Each item of the set is a tuple:\n",
    "    ((rotate_y, rotate_z), flip_x, flip_y, flip_z, transpose)\n",
    "\n",
    "    As an example, ((0, 1), 0, 1, 0, 1) represents a permutation in which the data is\n",
    "    rotated 90 degrees around the z-axis, then reversed on the y-axis, and then\n",
    "    transposed.\n",
    "\n",
    "    48 unique rotations & reflections:\n",
    "    https://en.wikipedia.org/wiki/Octahedral_symmetry#The_isometries_of_the_cube\n",
    "    \"\"\"\n",
    "    #집합을 반환\n",
    "    #itertools.product는 데카르트 곱을 의미\n",
    "    #즉, itertools.combinations_with_replacement(range(2), 2) 와 range(2), range(2), range(2), range(2), range(2)간에 일어날 수 있는\n",
    "    #모든 순열을 생성\n",
    "    #원래는 itertools.product( ,repeat=n) 값을 정해주어야 하나, repeat 값이 없으므로, 그냥 1\n",
    "    #즉 여기서는, 그냥,나열한것\n",
    "    \n",
    "    #ex)itertools.product(\"ABC\", 2)\n",
    "    #>>> AA, AB, AC, BA, BB, BC, CA, CB, CC\n",
    "    \n",
    "    #itertools.combinations_with_replacement은, itertools.product에서 AB와 BA를 같은것으로 보는것, 조합임\n",
    "    \n",
    "    #ex)itertools.combinations_with_replacement('ABC', 2)\n",
    "    #>>> AA AB AC BB BC CC\n",
    "\n",
    "    #즉, 얘를 풀면\n",
    "    #itertools.combinations_with_replacement(range(2), 2) = 00, 01, 11 3가지\n",
    "    #itertools.product([00, 01, 11], [0, 1], [0, 1], [0, 1], [0, 1])\n",
    "    #[]에서 하나씩 추출해서, 순열을 만든다. 그러면, 3*2*2*2*2 = 48가지가 나온다.\n",
    "    \n",
    "    return set(itertools.product(\n",
    "        itertools.combinations_with_replacement(range(2), 2), range(2), range(2), range(2), range(2)))\n",
    "\n",
    "#위의 list(generate_permutation_keys())에서 생성된 48개중 하나를 골라 반환\n",
    "def random_permutation_key():\n",
    "    \"\"\"\n",
    "    Generates and randomly selects a permutation key. See the documentation for the\n",
    "    \"generate_permutation_keys\" function.\n",
    "    \"\"\"\n",
    "    return random.choice(list(generate_permutation_keys()))\n",
    "\n",
    "\n",
    "#주어진 키의 특성에 따라 데이터를 바꿔준다. 인풋데이터 모양은 반드시, 다음 형태를 가져야 한다.(n_modalities, x, y, z)\n",
    "#인풋 키는 튜플로 다음과 같이 구성되어 있다. Input key is a tuple: (rotate_y, rotate_z), flip_x, flip_y, flip_z, transpose)\n",
    "#예를 들어, ((0, 1), 0, 1, 0, 1) 이것은, z축으로 90도 회전하고, y축 기준으로 반전시키고, transpose한것이다.\n",
    "\n",
    "def permute_data(data, key):\n",
    "    \"\"\"\n",
    "    Permutes the given data according to the specification of the given key. Input data\n",
    "    must be of shape (n_modalities, x, y, z).\n",
    "\n",
    "    Input key is a tuple: (rotate_y, rotate_z), flip_x, flip_y, flip_z, transpose)\n",
    "\n",
    "    As an example, ((0, 1), 0, 1, 0, 1) represents a permutation in which the data is\n",
    "    rotated 90 degrees around the z-axis, then reversed on the y-axis, and then\n",
    "    transposed.\n",
    "    \"\"\"\n",
    "    \n",
    "    #아래는, 튜플의 값에 따라, daata를 바꿔주는것을 구현한 코드\n",
    "    data = np.copy(data)\n",
    "    (rotate_y, rotate_z), flip_x, flip_y, flip_z, transpose = key\n",
    "\n",
    "    if rotate_y != 0:\n",
    "        data = np.rot90(data, rotate_y, axes=(1, 3))\n",
    "    if rotate_z != 0:\n",
    "        data = np.rot90(data, rotate_z, axes=(2, 3))\n",
    "    if flip_x:\n",
    "        data = data[:, ::-1]\n",
    "    if flip_y:\n",
    "        data = data[:, :, ::-1]\n",
    "    if flip_z:\n",
    "        data = data[:, :, :, ::-1]\n",
    "    if transpose:\n",
    "        for i in range(data.shape[0]):\n",
    "            data[i] = data[i].T\n",
    "    return data\n",
    "\n",
    "#위에서 정의한, random_permutation_key() 값인 키값을 받아, permute_data 함수를 수행하여, 데이터 변환\n",
    "#들어오는 데이터는 반드시, (n_modalities, x, y, z)의 형태를 가진, numpy array여야 할것\n",
    "def random_permutation_x_y(x_data, y_data):\n",
    "    \"\"\"\n",
    "    Performs random permutation on the data.\n",
    "    :param x_data: numpy array containing the data. Data must be of shape (n_modalities, x, y, z).\n",
    "    :param y_data: numpy array containing the data. Data must be of shape (n_modalities, x, y, z).\n",
    "    :return: the permuted data\n",
    "    \"\"\"\n",
    "    key = random_permutation_key()\n",
    "    return permute_data(x_data, key), permute_data(y_data, key)\n",
    "\n",
    "#permute_data(data, key) 함수를 적용하기 전으로 돌아감 \n",
    "def reverse_permute_data(data, key):\n",
    "    key = reverse_permutation_key(key)\n",
    "    data = np.copy(data)\n",
    "    (rotate_y, rotate_z), flip_x, flip_y, flip_z, transpose = key\n",
    "\n",
    "    if transpose:\n",
    "        for i in range(data.shape[0]):\n",
    "            data[i] = data[i].T\n",
    "    if flip_z:\n",
    "        data = data[:, :, :, ::-1]\n",
    "    if flip_y:\n",
    "        data = data[:, :, ::-1]\n",
    "    if flip_x:\n",
    "        data = data[:, ::-1]\n",
    "    if rotate_z != 0:\n",
    "        data = np.rot90(data, rotate_z, axes=(2, 3))\n",
    "    if rotate_y != 0:\n",
    "        data = np.rot90(data, rotate_y, axes=(1, 3))\n",
    "    return data\n",
    "\n",
    "#random_permutation_key()를 적용하기 전 상황으로 복구하는 키를 만드는 함수\n",
    "def reverse_permutation_key(key):\n",
    "    rotation = tuple([-rotate for rotate in key[0]])\n",
    "    return rotation, key[1], key[2], key[3], key[4]\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "#generator관련 함수가 정의되있다.\n",
    "\n",
    "#generator.py\n",
    "import os\n",
    "import copy\n",
    "from random import shuffle\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# from .utils import pickle_dump, pickle_load\n",
    "# from .utils.patches import compute_patch_indices, get_random_nd_index, get_patch_from_3d_data\n",
    "# from .augment import augment_data, random_permutation_x_y\n",
    "\n",
    "#training 과 validation generator 정의, 모델을 트레이닝 할때 사용\n",
    "#skip_blank \n",
    "def get_training_and_validation_generators(data_file, batch_size, n_labels, training_keys_file, validation_keys_file,\n",
    "                                           data_split=0.8, overwrite=False, labels=None, augment=False,\n",
    "                                           augment_flip=True, augment_distortion_factor=0.25, patch_shape=None,\n",
    "                                           validation_patch_overlap=0, training_patch_start_offset=None,\n",
    "                                           validation_batch_size=None, skip_blank=True, permute=False):\n",
    "    \"\"\"\n",
    "    Creates the training and validation generators that can be used when training the model.\n",
    "    :param skip_blank: If True, any blank (all-zero) label images/patches will be skipped by the data generator.\n",
    "    :param validation_batch_size: Batch size for the validation data.\n",
    "    :param training_patch_start_offset: Tuple of length 3 containing integer values. Training data will randomly be\n",
    "    offset by a number of pixels between (0, 0, 0) and the given tuple. (default is None)\n",
    "    :param validation_patch_overlap: Number of pixels/voxels that will be overlapped in the validation data. (requires\n",
    "    patch_shape to not be None)\n",
    "    :param patch_shape: Shape of the data to return with the generator. If None, the whole image will be returned.\n",
    "    (default is None)\n",
    "    :param augment_flip: if True and augment is True, then the data will be randomly flipped along the x, y and z axis\n",
    "    :param augment_distortion_factor: if augment is True, this determines the standard deviation from the original\n",
    "    that the data will be distorted (in a stretching or shrinking fashion). Set to None, False, or 0 to prevent the\n",
    "    augmentation from distorting the data in this way.\n",
    "    :param augment: If True, training data will be distorted on the fly so as to avoid over-fitting.\n",
    "    :param labels: List or tuple containing the ordered label values in the image files. The length of the list or tuple\n",
    "    should be equal to the n_labels value.\n",
    "    Example: (10, 25, 50)\n",
    "    The data generator would then return binary truth arrays representing the labels 10, 25, and 30 in that order.\n",
    "    :param data_file: hdf5 file to load the data from.\n",
    "    :param batch_size: Size of the batches that the training generator will provide.\n",
    "    :param n_labels: Number of binary labels.\n",
    "    :param training_keys_file: Pickle file where the index locations of the training data will be stored.\n",
    "    :param validation_keys_file: Pickle file where the index locations of the validation data will be stored.\n",
    "    :param data_split: How the training and validation data will be split. 0 means all the data will be used for\n",
    "    validation and none of it will be used for training. 1 means that all the data will be used for training and none\n",
    "    will be used for validation. Default is 0.8 or 80%.\n",
    "    :param overwrite: If set to True, previous files will be overwritten. The default mode is false, so that the\n",
    "    training and validation splits won't be overwritten when rerunning model training.\n",
    "    :param permute: will randomly permute the data (data must be 3D cube)\n",
    "    :return: Training data generator, validation data generator, number of training steps, number of validation steps\n",
    "    \"\"\"\n",
    "    if not validation_batch_size:\n",
    "        validation_batch_size = batch_size\n",
    "\n",
    "    training_list, validation_list = get_validation_split(data_file,\n",
    "                                                          data_split=data_split,\n",
    "                                                          overwrite=overwrite,\n",
    "                                                          training_file=training_keys_file,\n",
    "                                                          validation_file=validation_keys_file)\n",
    "\n",
    "    training_generator = data_generator(data_file, training_list,\n",
    "                                        batch_size=batch_size,\n",
    "                                        n_labels=n_labels,\n",
    "                                        labels=labels,\n",
    "                                        augment=augment,\n",
    "                                        augment_flip=augment_flip,\n",
    "                                        augment_distortion_factor=augment_distortion_factor,\n",
    "                                        patch_shape=patch_shape,\n",
    "                                        patch_overlap=0,\n",
    "                                        patch_start_offset=training_patch_start_offset,\n",
    "                                        skip_blank=skip_blank,\n",
    "                                        permute=permute)\n",
    "    validation_generator = data_generator(data_file, validation_list,\n",
    "                                          batch_size=validation_batch_size,\n",
    "                                          n_labels=n_labels,\n",
    "                                          labels=labels,\n",
    "                                          patch_shape=patch_shape,\n",
    "                                          patch_overlap=validation_patch_overlap,\n",
    "                                          skip_blank=skip_blank)\n",
    "\n",
    "    # Set the number of training and testing samples per epoch correctly\n",
    "    num_training_steps = get_number_of_steps(get_number_of_patches(data_file, training_list, patch_shape,\n",
    "                                                                   skip_blank=skip_blank,\n",
    "                                                                   patch_start_offset=training_patch_start_offset,\n",
    "                                                                   patch_overlap=0), batch_size)\n",
    "    print(\"Number of training steps: \", num_training_steps)\n",
    "\n",
    "    num_validation_steps = get_number_of_steps(get_number_of_patches(data_file, validation_list, patch_shape,\n",
    "                                                                     skip_blank=skip_blank,\n",
    "                                                                     patch_overlap=validation_patch_overlap),\n",
    "                                               validation_batch_size)\n",
    "    print(\"Number of validation steps: \", num_validation_steps)\n",
    "\n",
    "    return training_generator, validation_generator, num_training_steps, num_validation_steps\n",
    "\n",
    "\n",
    "def get_number_of_steps(n_samples, batch_size):\n",
    "    if n_samples <= batch_size:\n",
    "        return n_samples\n",
    "    elif np.remainder(n_samples, batch_size) == 0:\n",
    "        return n_samples//batch_size\n",
    "    else:\n",
    "        return n_samples//batch_size + 1\n",
    "\n",
    "\n",
    "def get_validation_split(data_file, training_file, validation_file, data_split=0.8, overwrite=False):\n",
    "    \"\"\"\n",
    "    Splits the data into the training and validation indices list.\n",
    "    :param data_file: pytables hdf5 data file\n",
    "    :param training_file:\n",
    "    :param validation_file:\n",
    "    :param data_split:\n",
    "    :param overwrite:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if overwrite or not os.path.exists(training_file):\n",
    "        print(\"Creating validation split...\")\n",
    "        nb_samples = data_file.root.data.shape[0]\n",
    "        sample_list = list(range(nb_samples))\n",
    "        training_list, validation_list = split_list(sample_list, split=data_split)\n",
    "        pickle_dump(training_list, training_file)\n",
    "        pickle_dump(validation_list, validation_file)\n",
    "        return training_list, validation_list\n",
    "    else:\n",
    "        print(\"Loading previous validation split...\")\n",
    "        return pickle_load(training_file), pickle_load(validation_file)\n",
    "\n",
    "\n",
    "def split_list(input_list, split=0.8, shuffle_list=True):\n",
    "    if shuffle_list:\n",
    "        shuffle(input_list)\n",
    "    n_training = int(len(input_list) * split)\n",
    "    training = input_list[:n_training]\n",
    "    testing = input_list[n_training:]\n",
    "    return training, testing\n",
    "\n",
    "\n",
    "def data_generator(data_file, index_list, batch_size=1, n_labels=1, labels=None, augment=False, augment_flip=True,\n",
    "                   augment_distortion_factor=0.25, patch_shape=None, patch_overlap=0, patch_start_offset=None,\n",
    "                   shuffle_index_list=True, skip_blank=True, permute=False):\n",
    "    orig_index_list = index_list\n",
    "    while True:\n",
    "        x_list = list()\n",
    "        y_list = list()\n",
    "        if patch_shape:\n",
    "            index_list = create_patch_index_list(orig_index_list, data_file.root.data.shape[-3:], patch_shape,\n",
    "                                                 patch_overlap, patch_start_offset)\n",
    "        else:\n",
    "            index_list = copy.copy(orig_index_list)\n",
    "\n",
    "        if shuffle_index_list:\n",
    "            shuffle(index_list)\n",
    "        while len(index_list) > 0:\n",
    "            index = index_list.pop()\n",
    "            add_data(x_list, y_list, data_file, index, augment=augment, augment_flip=augment_flip,\n",
    "                     augment_distortion_factor=augment_distortion_factor, patch_shape=patch_shape,\n",
    "                     skip_blank=skip_blank, permute=permute)\n",
    "            if len(x_list) == batch_size or (len(index_list) == 0 and len(x_list) > 0):\n",
    "                yield convert_data(x_list, y_list, n_labels=n_labels, labels=labels)\n",
    "                x_list = list()\n",
    "                y_list = list()\n",
    "\n",
    "\n",
    "def get_number_of_patches(data_file, index_list, patch_shape=None, patch_overlap=0, patch_start_offset=None,\n",
    "                          skip_blank=True):\n",
    "    if patch_shape:\n",
    "        index_list = create_patch_index_list(index_list, data_file.root.data.shape[-3:], patch_shape, patch_overlap,\n",
    "                                             patch_start_offset)\n",
    "        count = 0\n",
    "        for index in index_list:\n",
    "            x_list = list()\n",
    "            y_list = list()\n",
    "            add_data(x_list, y_list, data_file, index, skip_blank=skip_blank, patch_shape=patch_shape)\n",
    "            if len(x_list) > 0:\n",
    "                count += 1\n",
    "        return count\n",
    "    else:\n",
    "        return len(index_list)\n",
    "\n",
    "\n",
    "def create_patch_index_list(index_list, image_shape, patch_shape, patch_overlap, patch_start_offset=None):\n",
    "    patch_index = list()\n",
    "    for index in index_list:\n",
    "        if patch_start_offset is not None:\n",
    "            random_start_offset = np.negative(get_random_nd_index(patch_start_offset))\n",
    "            patches = compute_patch_indices(image_shape, patch_shape, overlap=patch_overlap, start=random_start_offset)\n",
    "        else:\n",
    "            patches = compute_patch_indices(image_shape, patch_shape, overlap=patch_overlap)\n",
    "        patch_index.extend(itertools.product([index], patches))\n",
    "    return patch_index\n",
    "\n",
    "\n",
    "def add_data(x_list, y_list, data_file, index, augment=False, augment_flip=False, augment_distortion_factor=0.25,\n",
    "             patch_shape=False, skip_blank=True, permute=False):\n",
    "    \"\"\"\n",
    "    Adds data from the data file to the given lists of feature and target data\n",
    "    :param skip_blank: Data will not be added if the truth vector is all zeros (default is True).\n",
    "    :param patch_shape: Shape of the patch to add to the data lists. If None, the whole image will be added.\n",
    "    :param x_list: list of data to which data from the data_file will be appended.\n",
    "    :param y_list: list of data to which the target data from the data_file will be appended.\n",
    "    :param data_file: hdf5 data file.\n",
    "    :param index: index of the data file from which to extract the data.\n",
    "    :param augment: if True, data will be augmented according to the other augmentation parameters (augment_flip and\n",
    "    augment_distortion_factor)\n",
    "    :param augment_flip: if True and augment is True, then the data will be randomly flipped along the x, y and z axis\n",
    "    :param augment_distortion_factor: if augment is True, this determines the standard deviation from the original\n",
    "    that the data will be distorted (in a stretching or shrinking fashion). Set to None, False, or 0 to prevent the\n",
    "    augmentation from distorting the data in this way.\n",
    "    :param permute: will randomly permute the data (data must be 3D cube)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data, truth = get_data_from_file(data_file, index, patch_shape=patch_shape)\n",
    "    if augment:\n",
    "        if patch_shape is not None:\n",
    "            affine = data_file.root.affine[index[0]]\n",
    "        else:\n",
    "            affine = data_file.root.affine[index]\n",
    "        data, truth = augment_data(data, truth, affine, flip=augment_flip, scale_deviation=augment_distortion_factor)\n",
    "\n",
    "    if permute:\n",
    "        if data.shape[-3] != data.shape[-2] or data.shape[-2] != data.shape[-1]:\n",
    "            raise ValueError(\"To utilize permutations, data array must be in 3D cube shape with all dimensions having \"\n",
    "                             \"the same length.\")\n",
    "        data, truth = random_permutation_x_y(data, truth[np.newaxis])\n",
    "    else:\n",
    "        truth = truth[np.newaxis]\n",
    "\n",
    "    if not skip_blank or np.any(truth != 0):\n",
    "        x_list.append(data)\n",
    "        y_list.append(truth)\n",
    "\n",
    "\n",
    "def get_data_from_file(data_file, index, patch_shape=None):\n",
    "    if patch_shape:\n",
    "        index, patch_index = index\n",
    "        data, truth = get_data_from_file(data_file, index, patch_shape=None)\n",
    "        x = get_patch_from_3d_data(data, patch_shape, patch_index)\n",
    "        y = get_patch_from_3d_data(truth, patch_shape, patch_index)\n",
    "    else:\n",
    "        x, y = data_file.root.data[index], data_file.root.truth[index, 0]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def convert_data(x_list, y_list, n_labels=1, labels=None):\n",
    "    x = np.asarray(x_list)\n",
    "    y = np.asarray(y_list)\n",
    "    if n_labels == 1:\n",
    "        y[y > 0] = 1\n",
    "    elif n_labels > 1:\n",
    "        y = get_multi_class_labels(y, n_labels=n_labels, labels=labels)\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_multi_class_labels(data, n_labels, labels=None):\n",
    "    \"\"\"\n",
    "    Translates a label map into a set of binary labels.\n",
    "    :param data: numpy array containing the label map with shape: (n_samples, 1, ...).\n",
    "    :param n_labels: number of labels.\n",
    "    :param labels: integer values of the labels.\n",
    "    :return: binary numpy array of shape: (n_samples, n_labels, ...)\n",
    "    \"\"\"\n",
    "    new_shape = [data.shape[0], n_labels] + list(data.shape[2:])\n",
    "    y = np.zeros(new_shape, np.int8)\n",
    "    for label_index in range(n_labels):\n",
    "        if labels is not None:\n",
    "            y[:, label_index][data[:, 0] == labels[label_index]] = 1\n",
    "        else:\n",
    "            y[:, label_index][data[:, 0] == (label_index + 1)] = 1\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T05:29:47.251088Z",
     "start_time": "2020-05-26T05:29:47.234087Z"
    }
   },
   "outputs": [],
   "source": [
    "#metrics.py\n",
    "from functools import partial\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "def dice_coefficient(y_true, y_pred, smooth=1.):\n",
    "    y_true_f = tf.cast(K.flatten(y_true), tf.float32, name=None)# K.flatten(y_true)\n",
    "    y_pred_f = tf.cast(K.flatten(y_pred), tf.float32, name=None)\n",
    "    #J.Lee: original: y_pred_f = K.flatten(y_pred)\n",
    "    #to prevent Value error. original y_pred is int8 and cannot be multiplied with fp32(y_true)\n",
    "    \n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "\n",
    "def dice_coefficient_loss(y_true, y_pred):\n",
    "    return -dice_coefficient(y_true, y_pred)\n",
    "\n",
    "\n",
    "def weighted_dice_coefficient(y_true, y_pred, axis=(-3, -2, -1), smooth=0.00001):\n",
    "    \"\"\"\n",
    "    Weighted dice coefficient. Default axis assumes a \"channels first\" data structure\n",
    "    :param smooth:\n",
    "    :param y_true:\n",
    "    :param y_pred:\n",
    "    :param axis:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return K.mean(2. * (K.sum(y_true * y_pred,\n",
    "                              axis=axis) + smooth/2)/(K.sum(y_true,\n",
    "                                                            axis=axis) + K.sum(y_pred,\n",
    "                                                                               axis=axis) + smooth))\n",
    "\n",
    "\n",
    "def weighted_dice_coefficient_loss(y_true, y_pred):\n",
    "    return -weighted_dice_coefficient(y_true, y_pred)\n",
    "\n",
    "\n",
    "def label_wise_dice_coefficient(y_true, y_pred, label_index):\n",
    "    return dice_coefficient(y_true[:, label_index], y_pred[:, label_index])\n",
    "\n",
    "\n",
    "def get_label_dice_coefficient_function(label_index):\n",
    "    f = partial(label_wise_dice_coefficient, label_index=label_index)\n",
    "    f.__setattr__('__name__', 'label_{0}_dice_coef'.format(label_index))\n",
    "    return f\n",
    "\n",
    "\n",
    "dice_coef = dice_coefficient\n",
    "dice_coef_loss = dice_coefficient_loss\n",
    "\n",
    "\n",
    "#training.py\n",
    "\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, CSVLogger, LearningRateScheduler, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "# from unet3d.metrics import (dice_coefficient, dice_coefficient_loss, dice_coef, dice_coef_loss,\n",
    "#                             weighted_dice_coefficient_loss, weighted_dice_coefficient)\n",
    "\n",
    "K.set_image_data_format('channels_first')\n",
    "# J.Lee: K.set_image_dim_ordering('th') -> K.set_image_data_format('channels_first') for use in keras ver2.x\n",
    "\n",
    "\n",
    "# learning rate schedule\n",
    "def step_decay(epoch, initial_lrate, drop, epochs_drop):\n",
    "    return initial_lrate * math.pow(drop, math.floor((1+epoch)/float(epochs_drop)))\n",
    "\n",
    "\n",
    "def get_callbacks(model_file, initial_learning_rate=0.0001, learning_rate_drop=0.5, learning_rate_epochs=None,\n",
    "                  learning_rate_patience=50, logging_file=\"training.log\", verbosity=1,\n",
    "                  early_stopping_patience=None):\n",
    "    callbacks = list()\n",
    "    callbacks.append(ModelCheckpoint(model_file, save_best_only=True))\n",
    "    callbacks.append(CSVLogger(logging_file, append=True))\n",
    "    if learning_rate_epochs:\n",
    "        callbacks.append(LearningRateScheduler(partial(step_decay, initial_lrate=initial_learning_rate,\n",
    "                                                       drop=learning_rate_drop, epochs_drop=learning_rate_epochs)))\n",
    "    else:\n",
    "        callbacks.append(ReduceLROnPlateau(factor=learning_rate_drop, patience=learning_rate_patience,\n",
    "                                           verbose=verbosity))\n",
    "    if early_stopping_patience:\n",
    "        callbacks.append(EarlyStopping(verbose=verbosity, patience=early_stopping_patience))\n",
    "    return callbacks\n",
    "\n",
    "\n",
    "def load_old_model(model_file):\n",
    "    print(\"Loading pre-trained model\")\n",
    "    custom_objects = {'dice_coefficient_loss': dice_coefficient_loss, 'dice_coefficient': dice_coefficient,\n",
    "                      'dice_coef': dice_coef, 'dice_coef_loss': dice_coef_loss,\n",
    "                      'weighted_dice_coefficient': weighted_dice_coefficient,\n",
    "                      'weighted_dice_coefficient_loss': weighted_dice_coefficient_loss}\n",
    "    try:\n",
    "        from keras_contrib.layers import InstanceNormalization\n",
    "        custom_objects[\"InstanceNormalization\"] = InstanceNormalization\n",
    "    except ImportError:\n",
    "        pass\n",
    "    try:\n",
    "        return load_model(model_file, custom_objects=custom_objects)\n",
    "    except ValueError as error:\n",
    "        if 'InstanceNormalization' in str(error):\n",
    "            raise ValueError(str(error) + \"\\n\\nPlease install keras-contrib to use InstanceNormalization:\\n\"\n",
    "                                          \"'pip install git+https://www.github.com/keras-team/keras-contrib.git'\")\n",
    "        else:\n",
    "            raise error\n",
    "\n",
    "\n",
    "def train_model(model, model_file, training_generator, validation_generator, steps_per_epoch, validation_steps,\n",
    "                initial_learning_rate=0.001, learning_rate_drop=0.5, learning_rate_epochs=None, n_epochs=500,\n",
    "                learning_rate_patience=20, early_stopping_patience=None):\n",
    "    \"\"\"\n",
    "    Train a Keras model.\n",
    "    :param early_stopping_patience: If set, training will end early if the validation loss does not improve after the\n",
    "    specified number of epochs.\n",
    "    :param learning_rate_patience: If learning_rate_epochs is not set, the learning rate will decrease if the validation\n",
    "    loss does not improve after the specified number of epochs. (default is 20)\n",
    "    :param model: Keras model that will be trained.\n",
    "    :param model_file: Where to save the Keras model.\n",
    "    :param training_generator: Generator that iterates through the training data.\n",
    "    :param validation_generator: Generator that iterates through the validation data.\n",
    "    :param steps_per_epoch: Number of batches that the training generator will provide during a given epoch.\n",
    "    :param validation_steps: Number of batches that the validation generator will provide during a given epoch.\n",
    "    :param initial_learning_rate: Learning rate at the beginning of training.\n",
    "    :param learning_rate_drop: How much at which to the learning rate will decay.\n",
    "    :param learning_rate_epochs: Number of epochs after which the learning rate will drop.\n",
    "    :param n_epochs: Total number of epochs to train the model.\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    model.fit(training_generator,\n",
    "                    steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=n_epochs,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=validation_steps,\n",
    "                    callbacks=get_callbacks(model_file,\n",
    "                                            initial_learning_rate=initial_learning_rate,\n",
    "                                            learning_rate_drop=learning_rate_drop,\n",
    "                                            learning_rate_epochs=learning_rate_epochs,\n",
    "                                            learning_rate_patience=learning_rate_patience,\n",
    "                                            early_stopping_patience=early_stopping_patience))\n",
    "# J.Lee:original:\n",
    "#     model.fit_generator(generator=training_generator,\n",
    "#                         steps_per_epoch=steps_per_epoch,\n",
    "#                         epochs=n_epochs,\n",
    "#                         validation_data=validation_generator,\n",
    "#                         validation_steps=validation_steps,\n",
    "#                         callbacks=get_callbacks(model_file,\n",
    "#                                                 initial_learning_rate=initial_learning_rate,\n",
    "#                                                 learning_rate_drop=learning_rate_drop,\n",
    "#                                                 learning_rate_epochs=learning_rate_epochs,\n",
    "#                                                 learning_rate_patience=learning_rate_patience,\n",
    "#                                                 early_stopping_patience=early_stopping_patience))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T05:29:48.760541Z",
     "start_time": "2020-05-26T05:29:48.744512Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#model/unet.py\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import Input, Model #J.Lee:original_code: from keras.engine import Input, Model\n",
    "from tensorflow.python.keras.layers import Conv3D, MaxPooling3D, UpSampling3D, Activation, BatchNormalization, PReLU, Conv3DTranspose\n",
    "#J.Lee:original_code: Deconvolution3D instead of Conv3DTranspose\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "#import tensorflow_addons as tfa #J.Lee: extracode added for InstanceNormalization \n",
    "\n",
    "# from unet3d.metrics import dice_coefficient_loss, get_label_dice_coefficient_function, dice_coefficient\n",
    "\n",
    "K.set_image_data_format(\"channels_first\")\n",
    "\n",
    "try:\n",
    "    from keras.engine import merge\n",
    "except ImportError:\n",
    "    from tensorflow.python.keras.layers import concatenate \n",
    "    #J.Lee:original_code: from keras.layers.merge import concatenate \n",
    "\n",
    "def unet_model_3d(input_shape, pool_size=(2, 2, 2), n_labels=1, initial_learning_rate=0.00001, deconvolution=False,\n",
    "                  depth=4, n_base_filters=32, include_label_wise_dice_coefficients=False, metrics=dice_coefficient,\n",
    "                  batch_normalization=False, activation_name=\"sigmoid\"):\n",
    "    \"\"\"\n",
    "    Builds the 3D UNet Keras model.f\n",
    "    :param metrics: List metrics to be calculated during model training (default is dice coefficient).\n",
    "    :param include_label_wise_dice_coefficients: If True and n_labels is greater than 1, model will report the dice\n",
    "    coefficient for each label as metric.\n",
    "    :param n_base_filters: The number of filters that the first layer in the convolution network will have. Following\n",
    "    layers will contain a multiple of this number. Lowering this number will likely reduce the amount of memory required\n",
    "    to train the model.\n",
    "    :param depth: indicates the depth of the U-shape for the model. The greater the depth, the more max pooling\n",
    "    layers will be added to the model. Lowering the depth may reduce the amount of memory required for training.\n",
    "    :param input_shape: Shape of the input data (n_chanels, x_size, y_size, z_size). The x, y, and z sizes must be\n",
    "    divisible by the pool size to the power of the depth of the UNet, that is pool_size^depth.\n",
    "    :param pool_size: Pool size for the max pooling operations.\n",
    "    :param n_labels: Number of binary labels that the model is learning.\n",
    "    :param initial_learning_rate: Initial learning rate for the model. This will be decayed during training.\n",
    "    :param deconvolution: If set to True, will use transpose convolution(deconvolution) instead of up-sampling. This\n",
    "    increases the amount memory required during training.\n",
    "    :return: Untrained 3D UNet Model\n",
    "    \"\"\"\n",
    "    inputs = Input(input_shape)\n",
    "    current_layer = inputs\n",
    "    levels = list()\n",
    "\n",
    "    # add levels with max pooling\n",
    "    for layer_depth in range(depth):\n",
    "        layer1 = create_convolution_block(input_layer=current_layer, n_filters=n_base_filters*(2**layer_depth),\n",
    "                                          batch_normalization=batch_normalization)\n",
    "        layer2 = create_convolution_block(input_layer=layer1, n_filters=n_base_filters*(2**layer_depth)*2,\n",
    "                                          batch_normalization=batch_normalization)\n",
    "        if layer_depth < depth - 1:\n",
    "            current_layer = MaxPooling3D(pool_size=pool_size)(layer2)\n",
    "            levels.append([layer1, layer2, current_layer])\n",
    "        else:\n",
    "            current_layer = layer2\n",
    "            levels.append([layer1, layer2])\n",
    "\n",
    "    # add levels with up-convolution or up-sampling\n",
    "    for layer_depth in range(depth-2, -1, -1):\n",
    "        up_convolution = get_up_convolution(pool_size=pool_size, deconvolution=deconvolution,\n",
    "                                            #J.Lee:original: n_filters=current_layer._keras_shape[1]\n",
    "                                            n_filters= tensorflow.keras.backend.int_shape(current_layer)[1])(current_layer)\n",
    "        concat = concatenate([up_convolution, levels[layer_depth][1]], axis=1)\n",
    "        current_layer = create_convolution_block(n_filters= tensorflow.keras.backend.int_shape(levels[layer_depth][1])[1],\n",
    "                                                 # J.Lee:original: levels[layer_depth][1]._keras_shape[1],\n",
    "                                                 input_layer=concat, batch_normalization=batch_normalization)\n",
    "        current_layer = create_convolution_block(n_filters= tensorflow.keras.backend.int_shape(levels[layer_depth][1])[1],\n",
    "                                                 # J.Lee:original: levels[layer_depth][1]._keras_shape[1],\n",
    "                                                 input_layer=current_layer,\n",
    "                                                 batch_normalization=batch_normalization)\n",
    "\n",
    "    final_convolution = Conv3D(n_labels, (1, 1, 1))(current_layer)\n",
    "    act = Activation(activation_name)(final_convolution)\n",
    "    model = Model(inputs=inputs, outputs=act)\n",
    "\n",
    "    if not isinstance(metrics, list):\n",
    "        metrics = [metrics]\n",
    "\n",
    "    if include_label_wise_dice_coefficients and n_labels > 1:\n",
    "        label_wise_dice_metrics = [get_label_dice_coefficient_function(index) for index in range(n_labels)]\n",
    "        if metrics:\n",
    "            metrics = metrics + label_wise_dice_metrics\n",
    "        else:\n",
    "            metrics = label_wise_dice_metrics\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=initial_learning_rate), loss=dice_coefficient_loss, metrics=metrics)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_convolution_block(input_layer, n_filters, batch_normalization=False, kernel=(3, 3, 3), activation=None,\n",
    "                             padding='same', strides=(1, 1, 1), instance_normalization=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param strides:\n",
    "    :param input_layer:\n",
    "    :param n_filters:\n",
    "    :param batch_normalization:\n",
    "    :param kernel:\n",
    "    :param activation: Keras activation layer to use. (default is 'relu')\n",
    "    :param padding:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    layer = Conv3D(n_filters, kernel, padding=padding, strides=strides)(input_layer)\n",
    "    if batch_normalization:\n",
    "        layer = BatchNormalization(axis=1)(layer)\n",
    "    elif instance_normalization:\n",
    "        try:\n",
    "            #J.Lee:original: from keras_contrib.layers.normalization import InstanceNormalization\n",
    "            from tensorflow_addons.layers import InstanceNormalization\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Install keras_contrib in order to use instance normalization.\"\n",
    "                              \"\\nTry: pip install git+https://www.github.com/farizrahman4u/keras-contrib.git\")\n",
    "        layer = InstanceNormalization(axis=1)(layer)\n",
    "    if activation is None:\n",
    "        return Activation('relu')(layer)\n",
    "    else:\n",
    "        return activation()(layer)\n",
    "\n",
    "\n",
    "def compute_level_output_shape(n_filters, depth, pool_size, image_shape):\n",
    "    \"\"\"\n",
    "    Each level has a particular output shape based on the number of filters used in that level and the depth or number \n",
    "    of max pooling operations that have been done on the data at that point.\n",
    "    :param image_shape: shape of the 3d image.\n",
    "    :param pool_size: the pool_size parameter used in the max pooling operation.\n",
    "    :param n_filters: Number of filters used by the last node in a given level.\n",
    "    :param depth: The number of levels down in the U-shaped model a given node is.\n",
    "    :return: 5D vector of the shape of the output node \n",
    "    \"\"\"\n",
    "    output_image_shape = np.asarray(np.divide(image_shape, np.power(pool_size, depth)), dtype=np.int32).tolist()\n",
    "    return tuple([None, n_filters] + output_image_shape)\n",
    "\n",
    "\n",
    "def get_up_convolution(n_filters, pool_size, kernel_size=(2, 2, 2), strides=(2, 2, 2),\n",
    "                       deconvolution=False):\n",
    "    if deconvolution:\n",
    "        return Conv3DTranspose(filters=n_filters, kernel_size=kernel_size,\n",
    "                               strides=strides) # J.Lee:original:Deconvolution3D\n",
    "    else:\n",
    "        return UpSampling3D(size=pool_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.set_image_data_format(\"channels_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T05:29:53.169554Z",
     "start_time": "2020-05-26T05:29:52.955523Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#brats/train.py\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import tensorflow\n",
    "# from unet3d.data import write_data_to_file, open_data_file\n",
    "# from unet3d.generator import get_training_and_validation_generators\n",
    "# from unet3d.model import unet_model_3d\n",
    "# from unet3d.training import load_old_model, train_model\n",
    "\n",
    "\n",
    "config = dict()\n",
    "config[\"pool_size\"] = (2, 2, 2)  # pool size for the max pooling operations\n",
    "config[\"image_shape\"] = (144, 144, 144)  # This determines what shape the images will be cropped/resampled to.\n",
    "config[\"patch_shape\"] = (64, 64, 64)  # switch to None to train on the whole image\n",
    "config[\"labels\"] = (1, 2, 4)  # the label numbers on the input image\n",
    "config[\"n_labels\"] = len(config[\"labels\"])\n",
    "config[\"all_modalities\"] = [\"flair\", \"t1\", \"t1ce\", \"t2\"]\n",
    "config[\"training_modalities\"] = config[\"all_modalities\"]  # change this if you want to only use some of the modalities\n",
    "config[\"nb_channels\"] = len(config[\"training_modalities\"])\n",
    "if \"patch_shape\" in config and config[\"patch_shape\"] is not None:\n",
    "    config[\"input_shape\"] = tuple([config[\"nb_channels\"]] + list(config[\"patch_shape\"]))\n",
    "else:\n",
    "    config[\"input_shape\"] = tuple([config[\"nb_channels\"]] + list(config[\"image_shape\"]))\n",
    "config[\"truth_channel\"] = config[\"nb_channels\"]\n",
    "config[\"deconvolution\"] = True  # if False, will use upsampling instead of deconvolution\n",
    "\n",
    "config[\"batch_size\"] = 6\n",
    "config[\"validation_batch_size\"] = 12\n",
    "config[\"n_epochs\"] = 500  # cutoff the training after this many epochs\n",
    "config[\"patience\"] = 10  # learning rate will be reduced after this many epochs if the validation loss is not improving\n",
    "config[\"early_stop\"] = 50  # training will be stopped after this many epochs without the validation loss improving\n",
    "config[\"initial_learning_rate\"] = 0.00001\n",
    "config[\"learning_rate_drop\"] = 0.5  # factor by which the learning rate will be reduced\n",
    "config[\"validation_split\"] = 0.8  # portion of the data that will be used for training\n",
    "config[\"flip\"] = False  # augments the data by randomly flipping an axis during\n",
    "config[\"permute\"] = True  # data shape must be a cube. Augments the data by permuting in various directions\n",
    "config[\"distort\"] = None  # switch to None if you want no distortion\n",
    "config[\"augment\"] = config[\"flip\"] or config[\"distort\"]\n",
    "config[\"validation_patch_overlap\"] = 0  # if > 0, during training, validation patches will be overlapping\n",
    "config[\"training_patch_start_offset\"] = (16, 16, 16)  # randomly offset the first patch index by up to this offset\n",
    "config[\"skip_blank\"] = True  # if True, then patches without any target will be skipped\n",
    "\n",
    "data_dir = '/home/lab10/MICCAI_BraTS_2019_Data_Training/'\n",
    "config[\"data_file\"] = data_dir + \"h5_outfiles.h5\" #os.path.abspath(\"brats_data.h5\")\n",
    "config[\"model_file\"] = data_dir + \"tumor_segmentation_model.h5\" #os.path.abspath(\"tumor_segmentation_model.h5\")\n",
    "config[\"training_file\"] = data_dir + \"training_ids.pkl\" #os.path.abspath(\"training_ids.pkl\")\n",
    "config[\"validation_file\"] = data_dir + \"validation_ids.pkl\" #os.path.abspath(\"validation_ids.pkl\")\n",
    "config[\"overwrite\"] = False  # If True, will previous files. If False, will use previously written files.\n",
    "\n",
    "\n",
    "def fetch_training_data_files():\n",
    "    training_data_files = list()\n",
    "    for subject_dir in glob.glob(os.path.join(os.path.dirname(\"__file__\"), \"data\", \"preprocessed\", \"*\", \"*\")):\n",
    "        subject_files = list()\n",
    "        for modality in config[\"training_modalities\"] + [\"truth\"]:\n",
    "            subject_files.append(os.path.join(subject_dir, modality + \".nii.gz\"))\n",
    "        training_data_files.append(tuple(subject_files))\n",
    "    return training_data_files\n",
    "\n",
    "\n",
    "# def main(overwrite=False):\n",
    "# convert input images into an hdf5 file\n",
    "overwrite = config[\"overwrite\"] \n",
    "\n",
    "if overwrite or not os.path.exists(config[\"data_file\"]):\n",
    "    training_files = fetch_training_data_files()\n",
    "\n",
    "    write_data_to_file(training_files, config[\"data_file\"], image_shape=config[\"image_shape\"])\n",
    "data_file_opened = open_data_file(config[\"data_file\"])\n",
    "\n",
    "if not overwrite and os.path.exists(config[\"model_file\"]):\n",
    "    model = load_old_model(config[\"model_file\"])\n",
    "else:\n",
    "    # instantiate new model\n",
    "    model = unet_model_3d(input_shape=config[\"input_shape\"],\n",
    "                          pool_size=config[\"pool_size\"],\n",
    "                          n_labels=config[\"n_labels\"],\n",
    "                          initial_learning_rate=config[\"initial_learning_rate\"],\n",
    "                          deconvolution=config[\"deconvolution\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-26T06:12:49.689933Z",
     "start_time": "2020-05-26T05:29:55.303008Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previous validation split...\n",
      "Number of training steps:  0\n",
      "Number of validation steps:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tables/leaf.py:414: PerformanceWarning: The Leaf ``/data`` is exceeding the maximum recommended rowsize (104857600 bytes);\n",
      "be ready to see PyTables asking for *lots* of memory and possibly slow\n",
      "I/O.  You may want to reduce the rowsize by trimming the value of\n",
      "dimensions that are orthogonal (and preferably close) to the *main*\n",
      "dimension of this leave.  Alternatively, in case you have specified a\n",
      "very small/large chunksize, you may want to increase/decrease it.\n",
      "  PerformanceWarning)\n"
     ]
    }
   ],
   "source": [
    "# get training and testing generators\n",
    "train_generator, validation_generator, n_train_steps, n_validation_steps = get_training_and_validation_generators(\n",
    "    data_file_opened,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    data_split=config[\"validation_split\"],\n",
    "    overwrite=overwrite,\n",
    "    validation_keys_file=config[\"validation_file\"],\n",
    "    training_keys_file=config[\"training_file\"],\n",
    "    n_labels=config[\"n_labels\"],\n",
    "    labels=config[\"labels\"],\n",
    "    patch_shape=config[\"patch_shape\"],\n",
    "    validation_batch_size=config[\"validation_batch_size\"],\n",
    "    validation_patch_overlap=config[\"validation_patch_overlap\"],\n",
    "    training_patch_start_offset=config[\"training_patch_start_offset\"],\n",
    "    permute=config[\"permute\"],\n",
    "    augment=config[\"augment\"],\n",
    "    skip_blank=config[\"skip_blank\"],\n",
    "    augment_flip=config[\"flip\"],\n",
    "    augment_distortion_factor=config[\"distort\"]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--sys.version—\n",
      "3.6.5 |Anaconda, Inc.| (default, Apr 29 2018, 16:14:56) \n",
      "[GCC 7.2.0]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"--sys.version—\")\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-26T05:30:57.595Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-63de22a7d7f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mlearning_rate_patience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"patience\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mearly_stopping_patience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"early_stop\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             n_epochs=config[\"n_epochs\"])\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mdata_file_opened\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-daa185fcf42e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, model_file, training_generator, validation_generator, steps_per_epoch, validation_steps, initial_learning_rate, learning_rate_drop, learning_rate_epochs, n_epochs, learning_rate_patience, early_stopping_patience)\u001b[0m\n\u001b[1;32m    142\u001b[0m                                             \u001b[0mlearning_rate_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                                             \u001b[0mlearning_rate_patience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate_patience\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                                             early_stopping_patience=early_stopping_patience))\n\u001b[0m\u001b[1;32m    145\u001b[0m \u001b[0;31m# J.Lee:original:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;31m#     model.fit_generator(generator=training_generator,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, standardize_function, workers, use_multiprocessing, max_queue_size, **kwargs)\u001b[0m\n\u001b[1;32m    745\u001b[0m     \u001b[0;31m# Since we have to know the dtype of the python generator when we build the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m     \u001b[0;31m# dataset, we have to look at a batch to infer the structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 747\u001b[0;31m     \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    748\u001b[0m     \u001b[0massert_not_namedtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_peek_and_restore\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    848\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_peek_and_restore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 850\u001b[0;31m     \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpeek\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpeek\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a80b2ddb1e93>\u001b[0m in \u001b[0;36mdata_generator\u001b[0;34m(data_file, index_list, batch_size, n_labels, labels, augment, augment_flip, augment_distortion_factor, patch_shape, patch_overlap, patch_start_offset, shuffle_index_list, skip_blank, permute)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0my_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpatch_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m             index_list = create_patch_index_list(orig_index_list, data_file.root.data.shape[-3:], patch_shape,\n\u001b[0m\u001b[1;32m    448\u001b[0m                                                  patch_overlap, patch_start_offset)\n\u001b[1;32m    449\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tables/group.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_g_add_children_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_f_get_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tables/group.py\u001b[0m in \u001b[0;36m_f_get_child\u001b[0;34m(self, childname)\u001b[0m\n\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 697\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m_f_get_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchildname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    698\u001b[0m         \"\"\"Get the child called childname of this group.\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from tensorflow.keras.mixed_precision import experimental as mixed_precision\n",
    "# policy = mixed_precision.Policy('mixed_float16')\n",
    "# mixed_precision.set_policy(policy)\n",
    "# print('Compute dtype: %s' % policy.compute_dtype)\n",
    "# print('Variable dtype: %s' % policy.variable_dtype)\n",
    "\n",
    "# run training\n",
    "train_model(model=model,\n",
    "            model_file=config[\"model_file\"],\n",
    "            training_generator=train_generator,\n",
    "            validation_generator=validation_generator,\n",
    "            steps_per_epoch=n_train_steps,\n",
    "            validation_steps=n_validation_steps,\n",
    "            initial_learning_rate=config[\"initial_learning_rate\"],\n",
    "            learning_rate_drop=config[\"learning_rate_drop\"],\n",
    "            learning_rate_patience=config[\"patience\"],\n",
    "            early_stopping_patience=config[\"early_stop\"],\n",
    "            n_epochs=config[\"n_epochs\"])\n",
    "data_file_opened.close()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main(overwrite=config[\"overwrite\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-25T11:28:12.011499Z",
     "start_time": "2020-05-25T11:28:12.007505Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
